<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Improved Contrastive Divergence Training of Energy Based Models">
    <meta name="author"
          content="Yilun Du, Shuang Li, Joshua B. Tenenbaum, Igor Mordatch">

    <title>Improved Contrastive Divergence Training of Energy Based Models</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Improved Contrastive Divergence Training of Energy Based Models</h2>
    <h3>ICML 2021</h3>
    <hr>
    <p class="authors">
        <a href="https://yilundu.github.io">Yilun Du</a>,
        <a href="https://people.csail.mit.edu/lishuang/"> Shuang Li</a>,
        <a href="https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/"> Joshua B. Tenenbaum</a>,
        <a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en">Igor Mordatch</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2012.01316">Paper</a>
        <a class="btn btn-primary" href="https://github.com/yilundu/improved_contrastive_divergence">Code</a>
    </div>
</div>


<div class="container">
    <div class="section">
        <p>
            We propose several different techniques to improve contrastive divergence training of energy-based models (EBMs). We first show that a gradient term neglected in the popular contrastive divergence formulation is both tractable to estimate and is important to avoid training instabilities in previous models. We further highlight how data augmentation, multi-scale processing, and reservoir sampling can be used to improve model robustness and generation quality. Thirdly, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.
        </p>
    </div>

    <div class="section">
        <h2>Improved Training of Energy-Based Models</h2>
        <hr>
        <p>
			Energy-Based Models (EBMs) represent the likelihood of a probability distribution of data by assigning an unnormalized probability scalar (or "energy") to each input data point. This provides significant model flexibility; any arbitrary model that outputs a real number can be used as an energy model. A difficulty however, is that training EBMs is hard, as to properly maximize the likelihood of an energy function, samples must be drawn from the energy model.

		In this work we present a set of improvements to contrastive divergence training of EBMs, enabling more stable, high resolution generation with EBMs. In particular we propose to:
		</p>

		<ul id='cd'>
		<li font-size: 6px>
		Add a KL loss term into contrastive divergence, which corresponds to a typically ignored gradient. We show that this significantly stabilizes and improves generative performance.
		</li>

		<li font-size: 6px>
		Integrate data-augmentation transitions while training EBMs to encourage mode mixing between model samples.
		</li>

		<li font-size: 6px>
		Factorize generation to a set of multi-scale energy functions operating on the input.
		</li>
		</ul>
    </div>

    <div class="section">
        <h2>Contrastive Divergence</h2>
        <hr>
        <p>
            A common objective used to train EBMs is <a =href="https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf">contrastive divergence</a>. Contrastive divergence consists of the following objective:
            \[ \text{KL}(p_D(x) \ || \  p_{\theta} (x)) - \text{KL}(q_{\theta}(x) \ || \  p_{\theta}(x)).  \]
            This objective minimizes the difference between the KL divergence of the data distribution and EBM distribution, and the KL divergence of finite number of MCMC steps on data distribution and EBM distribution.
            This objective has a key gradient (highlighted in red) that is often ignored.

            \[      - \left ( \mathbb{E}_{p_D(x)} \left [\frac{\partial E_{\theta}(x)}{\partial \theta} \right ] - \mathbb{E}_{q_{\theta}(x')} [\frac{\partial E_{\theta}(x')}{\partial \theta}] 
            + {\color{red} \frac{\partial q(x')}{\partial \theta} \frac{\partial \text{KL}(q_{\theta}(x') \ || \  p_{\theta}(x'))}{\partial q_{\theta}(x')} } \right ) \]

            We present a KL loss to capture this gradient (see our paper for details). We find that this missing 
            gradient contributes substantially to the overall training gradient of a EBM. The addition of our proposed
            KL loss  significantly stablizes training of EBMs and enables us to add additional architectural blocks to
            training.
        </p>


    </div>

    <div class="section">
        <h2>Data Augmentation</h2>
        <hr>
        <p>
            A difficulty when training EBMs is that underlying MCMC chains fail to mix and cover the EBM distribution. 
            To enable more effective mixing of MCMC chains, we intersperse Langevin sampling with data augmentation 
            transitions. This enables image sampling chains from our model to travel across large number of modes in
            the energy landscape. We illustrate the underlying sampling process below:
        </p>
            
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src='fig/langevin_data.gif' class='img-fluid'>
            </div>
        </div>

        <!---
        <p>
            When comparing samples initialized from the same random noise value, we find that the addition of data 
            augmentation enables significantly more diverse samples than without. 
        </p>

        <div class="row justify-content-left">
            <div class="col-sm-2">
			</div>
            <div class="col-sm-8">
                <img src='fig/diverse_sample.gif' class='img-fluid'>
            </div>
            <div class="col-sm-2">
			</div>
        </div>
        -->

    </div>

    <div class="section">
        <h2>Zero Shot Compositional Generation</h2>
        <hr>
        <p>
			EBMs are able to independently compose with other <a href="https://energy-based-model.github.io/compositional-generation-inference/">EBMs</a>, allowing us flexibly compose different models. We show that our approach enables higher resolution compositional generation across different domains. We independently train EBMs for CelebA factors of age, gender, smiling, and wavy hair. We show below that by adding each energy model in generation, we are able to gradually able to construct and change generations to exhibit each desired factor, as encoded by an individual energy function.
        </p>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="1080" height="360" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="fig/comp_face.mp4" type="video/mp4">
					 Your browser does not support the video tag.
                </video>
            </div>
        </div>

		<p style="font-size:14px">
		We can further independently train EBMs on rendering attributes of shape, size, position and rotation. By adding independent energy model in generation, we are also able to gradually construct generations that exhibit each desired factor.
		</p>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="1080" height="360" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="fig/comp_blender.mp4" type="video/mp4">
					 Your browser does not support the video tag.
                </video>
            </div>
        </div>


    </div>

    <div class="section">
        <h2>Out of Distribution Robustness</h2>
        <hr>
        <p>
			Another interesting property of EBMs is their ability to identify <a href="https://openai.com/blog/energy-based-models/">out-of-distribution samples</a> not in the training data distribution by utilizing the output
            predicted energy. We find that our approach significantly outperforms other EBMs on the task of identifying
            out-of-distribution data samples.
        </p>

        <div class="row justify-content-left">
            <div class="col-sm-3">
			</div>
            <div class="col-sm-6">
                <img src="fig/ood.png" style="width:100%">
			</div>
            <div class="col-sm-3">
			</div>
		</div>

    </div>

        <div class="section">
            <h2>Related Projects</h2>
            <hr>
            <p>
                Check out our related projects on utilizing energy based models! <br>
            </p>

            <div class='row vspace-top'>
		    <div class="col-sm-3">
			<img src='related_works/landmap_short.png' class='img-fluid'>
		    </div>

		    <div class="col">
			<div class='paper-title'>
			    <a href="https://energy-based-model.github.io/Energy-Based-Models-for-Continual-Learning/">Energy-Based Models for Continual Learning</a>
			</div>
			<div>
			    We motivate Energy-Based Models (EBMs) as a promising model class for continual learning problems. Instead of tackling continual learning via the use of external memory, growing models, or regularization, EBMs change the underlying training objective to causes less interference with previously learned information. Our proposed version of EBMs for continual learning is simple, efficient, and outperforms baseline methods by a large margin on several benchmarks. 
			    We further show that EBMs are adaptable to a more general continual learning setting where the data distribution changes without the notion of explicitly delineated tasks. 
			</div>
		    </div>
		</div>
		
		
	    <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/comp_cartoon.png' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://energy-based-model.github.io/compositional-generation-inference/">Compositional Visual Generation with Energy Based Models</a>
                    </div>
                    <div>
                        We show how EBMs enable <b>zero-shot compositional</b> visual generation, enabling us to compose visual concepts
                        (through operators of conjunction, disjunction, or negation) together in a zero-shot manner. 
                        Our approach enables us to generate faces given a  description 
                        ((Smiling AND Female) OR (NOT Smiling AND Male)) or to combine several different objects together.
                    </div>
                </div>
            </div>


            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/protein.png' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://arxiv.org/abs/2004.13167">Energy Based Models for Atomic Level Protein Conformations</a>

                    </div>
                    <div>
                        We introduce EBMs for modeling the underlying energy landscape of atomic level protein conformations. We train
                        EBMs to predict the energy of different protein rotamer configurations, and find that our trained EBM models 
                        can nearly match the performance of classical energy function Rosetta on the task of protein sidechain prediction.
                    </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/ebm_plan.png' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://arxiv.org/abs/1909.06878">Model Based Planning with Energy Based Models</a>
                    </div>
                    <div>
                        We present a framework towards utilizing EBMs to learn, in an online fashion, trajectory level plans for 
                        different start and goal configurations. This allows us to flexibly change and adapt to different
                        sets of goals by changing the underlying trajectory inference objective.
                    </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/half.mp4" type="video/mp4">
                    </video>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://openai.com/blog/energy-based-models/">Implicit Generation and Generalization with Energy Based Models</a>

                    </div>
                    <div>
                        We introduce a method to scale EBM training to modern neural network architectures.
                        We show that such trained EBMs have a set of unique properties, enabling model robustness,
                        image and trajectory modeling, continual learning and compositional visual generation.
                    </div>
                </div>
            </div>


            <div class="section">
                <h2>Paper</h2>
                <hr>
                <div>
                    <div class="list-group">
                        <a href="https://arxiv.org/abs/2012.01316"
                           class="list-group-item">
                            <img src="img/paper_thumbnail.png"
                                 style="width:100%; margin-right:-20px; margin-top:-10px;">
                        </a>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>Bibtex</h2>
                <hr>
                <div class="bibtexsection">
                    @inproceedings{du2021improved,
                      title={Improved Contrastive Divergence Training of Energy Based Models},
                      author={Du, Yilun and Li, Shuang and Tenenbaum, B. Joshua and Mordatch, Igor},
                      journal={Proceedings of the 38th International Conference on Machine
                    Learning (ICML-21)},
                      year={2021}
                    }
                </div>
            </div>

            <hr>

            <footer>
                <p>Send feedback and questions to <a href="https://yilundu.github.io">Yilun Du</a></p>
            </footer>
        </div>

</body>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>
